{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infectious-produce",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://tensorflowkorea.files.wordpress.com/2020/12/4.-e18492e185a9e186abe1848ce185a1-e18480e185a9e186bce18487e185aee18492e185a1e18482e185b3e186ab-e18486e185a5e18489e185b5e186abe18485e185a5e18482e185b5e186bce18483e185b5e186b8e18485e185a5e.png?w=972\" width=\"200\" height=\"200\"><br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-advocate",
   "metadata": {},
   "source": [
    "## 04-2 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-retailer",
   "metadata": {},
   "source": [
    "### - 점진적인 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-belfast",
   "metadata": {},
   "source": [
    "이번에 배울 내용은 훈련데이터가 조금씩 전달된다는 가정하에 시작하겠다. 조금씩 전달된 새로운 데이터를 기존의 훈련 데이터에 계속 추가하여 모델을 매일매일 훈련하면 어떨까? 나쁘지 않은 방법이다. 하지만 매일 추가되는 새로운 데이터를 활용해 모델을 훈련하면 데이터가 계속 늘어난다. 몇달이 지나면 모델을 훈련하기 위해 서버를 늘려야 한다. 이것은 지속 가능한 방법은 아니다.\n",
    "다른 방법은 새로운 데이터를 추가할 때 이전 데이터를 버림으로써 훈련 데이터 크기를 일정하게 유지하는 것이다. 이렇게 하면 데이터셋의 크기가 너무 커지지 않을 수 있다. 하지만 데이터를 버릴 때 다른 데이터에 없는 중요한 생선 데이터가 포함되어 있으면 큰일이다. \n",
    "앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 훈련하는 방식을 점진적 학습이라고 한다. 대표적인 점진적 학습 알고리즘은 확률적 경사 하강법이다. 사이킷런에서 확률적 경사 하강법을 위한 클래스를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-jamaica",
   "metadata": {},
   "source": [
    "### - 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-porter",
   "metadata": {},
   "source": [
    "확률적 경사 하강법에서 확률적이란 말은 '무작위' 혹은 '랜덤하게'의 기술적 표현이다. 경사는 기울기를 얘기하고 하강법은 내려가는 방법이다. 산에서 내려온다 가정해보면 집으로 가기위해 등산로 입구까지 내려가야 한다. 가장 빠른길은 경사가 가장 가파른 길이다. 경사 하강법이 바로 이런 방식이다. 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표이다. 하지만 다리가 길어 한번에 걸음이 너무 크면 경사를 따라 내려가지 못하고, 오히려 올라갈 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-mining",
   "metadata": {},
   "source": [
    "실제로 산에서 내려올 때는 천천히 조금씩 내려와야 한다. 경사 하강법은 가장 가파른 길을 찾아 내려오지만 조금씩 내려오는 것이 중요하다. 이렇게 내려오는 과정이 바로 경사 하강법 모델을 훈련하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-intelligence",
   "metadata": {},
   "source": [
    "이번엔 확률적이란 말을 이해해보자. 경사 하강법으로 내려올 때 가장 가파른 길을 찾는 방법은 무엇일까? 훈련 세트를 사용해 모델을 훈련하기 때문에 경사 하강법도 당연히 훈련 세트를 사용하여 가장 가파른 길을 찾을 것이다. 그런데 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾는다. 이처럼 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것이 바로 확률적 경사 하강법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-intellectual",
   "metadata": {},
   "source": [
    "모든 샘플을 다 사용하면 다시 처음부터 시작한다. 훈련 세트에 모든 샘플을 다시 채워 넣는다. 그다음 다시 랜덤하게 하나의 샘플을 선택해 이어서 경사를 내려간다. 이렇게 만족할 위치에 도달할 때까지 계속 내려가면 된다. 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정을 에포트(epoch) 라고 부른다. 일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-operations",
   "metadata": {},
   "source": [
    "그리고 여러개의 샘플을 사용해 경사 하강법을 수행하는 방식을 미치배치 경사 하강법이라고 한다. 또한 극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용할 수도 있다. 이를 배치 경사 하강법 이라고 부른다. 사실 전체 데이터를 사용하기 때문에 가장 안정적인 방법이다. 하지만 전체 데이터를 사용하면 그만큼 컴퓨터 자원을 많이 사용하게 된다. 어떤 경우는 데이터가 너무많아 한번에 전체 데이터를 모두 읽을수 없을지도 모른다. 확률적 경사 하강법은 훈련 세트를 사용해 산 아래에 있는 최적의 장소로 조금씩 이동하는 알고리즘이다.\n",
    "이 때문에 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트되어도 학습을 계속 이어나갈 수 있다. 즉 다시 산꼭대기에서부터 시작할 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-member",
   "metadata": {},
   "source": [
    "하지만 어디서 내려가야 하는 걸까? 다시 말해 가장 빠른 길을 찾아 내려가려고 하는 이 산은 도대체 무엇일까? 이 산을 손실 함수라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-eating",
   "metadata": {},
   "source": [
    "### - 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-pantyhose",
   "metadata": {},
   "source": [
    "손실 함수(loss function)는 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이다. 그렇다면 손실함수는 값이 작을수록 좋다. 하지만 어떤 값이 최솟값인지는 모른다. 가능한 많이 찾아보고 만족할 만한 수준이라면 산을 다 내려왔다고 인정해야 한다. 이 값을 찾아 조금씩 이동하려면 확률적 경사 하강법이 잘 맞을듯 하다. 다행히 우리가 다루는 많은 문제에 필요한 손실 함수는 이미 정의되어 있다.\n",
    "그럼 생선을 분류하기 위해서는 어떤 손실 함수를 사용하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-fashion",
   "metadata": {},
   "source": [
    "분류에서 손실은 아주 확실하다. 정답을 못 맞히는 것이다. 이진 분류를 예를 들어보면 도미는 1, 빙어는 0이다. 만약 4개의 예측 중 2개만 맞았으면 정확도는 1/2 = 0.5 이다. 하지만 정확도에 치명적인 단점은 4개의 샘플만 있다는 가정하에 가능한 정확도는 0, 0.25, 0.5, 0.75, 1 다섯가지 뿐이다. 앞서 경사 하강법을 사용할 때 아주 조금씩 내려온다고 했었다. 하지만 정확도가 이렇게 듬성듬성하면 경사 하강법을 이용해 조금씩 내려올 수 없다. 경사면은 확실히 연속적이어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-sample",
   "metadata": {},
   "source": [
    "앞서 로지스틱 회귀에서 로지스틱 회귀 모델이 확률을 출력했었다. 예측은 0 또는 1이지만 확률은 0 ~ 1 사이의 어떤 값도 될 수 있다. 즉 연속적이다. 예를 들어 위의 샘플 4개의 예측 확률을 각각 0.9, 0.3, 0.2, 0.8 이라고 가정해보자. 첫 번째 샘플부터 하나씩 어떻게 손실 함수를 만들 수 있는지 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-teens",
   "metadata": {},
   "source": [
    "### - 로지스틱 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-literature",
   "metadata": {},
   "source": [
    "첫 번째 샘플의 예측은 0.9 이므로 양성 클래스의 타겟인 1과 곱한 다음 음수로 바꿀 수 있다. 이 경우 예측이 1에 가까울수록 좋은 모델이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
