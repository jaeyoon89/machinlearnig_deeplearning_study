{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103ddcd7",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://tensorflowkorea.files.wordpress.com/2020/12/4.-e18492e185a9e186abe1848ce185a1-e18480e185a9e186bce18487e185aee18492e185a1e18482e185b3e186ab-e18486e185a5e18489e185b5e186abe18485e185a5e18482e185b5e186bce18483e185b5e186b8e18485e185a5e.png?w=972\" width=\"200\" height=\"200\"><br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9c3aa",
   "metadata": {},
   "source": [
    "# Chapter.9 텍스트를 위한 인공 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022913f7",
   "metadata": {},
   "source": [
    "## 09-1 순차 데이터와 순차 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594ec16",
   "metadata": {},
   "source": [
    "### - 순차 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d816fd",
   "metadata": {},
   "source": [
    "순차 데이터는 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터를 말한다. 예를 들면 일별 온도를 기록한 데이터에서 날짜 순서를 뒤죽박죽 섞는다면 내일의 온도를 쉽게 예상하기 어렵다. 지금까지 우리가 보았던 데이터는 순서와는 상관이 없었다. 예를 들어 패션 MNIST 데이터를 생각해보자. 이 데이터를 신경망 모델에 전달할 때 샘플을 랜덤하게 섞은 후 훈련 세트와 검증 세트로 나누었다. 즉 샘플의 순서는 상관이 없었다. 심지어 골고루 섞는 편이 결과가 더 좋았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9fdfd",
   "metadata": {},
   "source": [
    "이 장에서 사용하려는 댓글, 즉 텍스트 데이터는 단어의 순서가 중요한 순차 데이터이다. 이런 데이터는 순서를 유지하며 신경망에 주입해야 한다.따라서 순차 데이터를 다룰 때는 이전에 입력한 데이터를 기억하는 기능이 필요하다. 예를 들어 '별로지만 추천해요' 에서 '추천해요'가 입력될 때 '별로지만'을 기억하고 있어야 이 댓글을 무조건 긍정적이라고 판단하지 않을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19bf7",
   "metadata": {},
   "source": [
    "완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없다. 하나의 샘플을 사용하여 정방향 계산을 수행하고 나면 그 샘플은 버려지고 다음 샘플을 처리할 때 재사용하지 않는다. 이렇게 입력 데이터의 흐름이 앞으로만 전달되는 신경망을 피드포워드 신경망이라고 한다. 이전에 배웠던 완전 연결 신경망과 합성곱 신경망이 모두 피드포워드 신경망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62582a",
   "metadata": {},
   "source": [
    "신경망이 이전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용하기 위해서는 이렇게 데이터 흐름이 앞으로만 전달 되어서는 안된다. 다음 샘플을 위해 이전 데이터가 신경망 층에 순환될 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52cc50",
   "metadata": {},
   "source": [
    "### - 순환 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f421a",
   "metadata": {},
   "source": [
    "순환 신경망은 일반적인 완전 연결 신경망과 거의 비슷하다. 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리 하나만 추가하면 된다. 순환 신경망에서는 '이전 샘플에 대한 기억을 가지고 있다'고 종종 말한다. 이렇게 샘플을 처리하는 한 단계를 타임스텝이라고 한다. 순환 신경망은 이전 타임스텝의 샘플을 기억하지만 타임스텝이 오래될수록 순환되는 정보는 희미해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686c73e",
   "metadata": {},
   "source": [
    "순환 신경망에서는 특별히 층을 셀이라고 부른다. 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현한다. 또 셀의 출력을 은닉 상태라고 부른다. 일반적으로 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트 함수가 많이 사용된다. tanh 함수도 S자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 한다. tanh 함수는 시그모이드 함수와 달리 -1 ~ 1 사이의 범위를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9fbd4e",
   "metadata": {},
   "source": [
    "다른 신경망과 마찬가지로 순환 신경망에도 활성화 함수가 반드시 필요하다. 순환 신경망의 뉴런은 가중치가 하나 더 있다. 바로 이전 타임스텝의 은닉 상태에 곱해지는 가중치이다. 셀은 입력과 이전 타임스텝의 은닉 상태를 사용하여 현재 타임스텝의 은닉 상태를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed3714",
   "metadata": {},
   "source": [
    "### - 셀의 가중치와 입출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135e791",
   "metadata": {},
   "source": [
    "순환 신경망의 셀에서 필요한 가중치 크기를 계산해 보자. 복잡한 모델을 배울수록 가중치 개수를 계산해 보면 잘 이해하고 있는지 알 수 있다. 만약 순환층에 입력되는 특성의 개수가 4개이고 순환층의 뉴런이 3개라고 가정해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdd605",
   "metadata": {},
   "source": [
    "wx의 크기는 입력층과 순환층의 뉴런이 모두 연결되기 때문에 가중치 wx의 크기는 4 x 3 = 12가 된다. 그럼 순환층에서 다음 타임스텝에 재사용되는 은닉 상태를 위한 가중치 wh의 크기는 어떻게 될까? 순환층에 있는 첫 번째 뉴런의 은닉 상태가 다음 타임스텝에 재사용될 때 첫 번째 뉴런과 두 번쨰 뉴런, 세 번째 뉴런에 모두 전달된다. 따라서 이 순환층에서 은닉상태를 위한 가중치 wh는 3 x 3 = 9개 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b533d",
   "metadata": {},
   "source": [
    "이번에는 파라미터의 개수를 계산해보자. 가중치에 절편을 더하면 된다. 여기엔 각 뉴런마다 하나의 절편이 있다. 따라서 이 순환층은 모두 12 + 9 + 3 = 24개의 모델 파라미터를 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31c919",
   "metadata": {},
   "source": [
    "순환층은 일반적으로 샘플마다 2개의 차원을 가진다. 보통 하나의 샘플을 하나의 시퀀스라고 말한다. 시퀀스 안에는 여러 개의 아이템이 들어있다. 여기에서 시퀀스의 길이가 바로 타임스텝의 길이가 된다. 하나의 샘플은 시퀀스 길이와 단어 표현의 2차원 배열이다. 순환층을 통과하면 1차원 배열로 바뀐다. 이 1차원 배열의 크기는 순환층의 뉴런 개수에 의해 결정된다. 사실 순환층은 기본적으로 마지막 타임스텝의 은닉 상태만 출력으로 내보낸다. 이는 마치 입력된 시퀀스 길이를 모두 읽어서 정보를 마지막 은닉 상태에 압축하여 전달하는 것처럼 볼 수 있다. 이제 순환 신경망이 정보를 기억하는 메모리를 가진다고 표현하는지 이해할 수 있다. 또 순환 신경망이 순차 데이터에 잘 맞는 이유를 파악할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bf5ee",
   "metadata": {},
   "source": [
    "순환 신경망도 완전 연결망이나 합성곱 신경망처럼 여러 개의 층을 쌓을 수 있다. 순환층을 여러 개 쌓았을 때는 셀의 출력은 어떻게 달라질까? 셀의 입력은 샘플마다 타임스텝과 단어 표현으로 이루어진 2차원 배열이어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c0521",
   "metadata": {},
   "source": [
    "출력층의 구성에 대해 알아보자. 합성곱 신경망과 마찬가지로 순환 신경망도 마지막에는 밀집층을 두어 클래스를 분류한다. 다중 분류일 경우에는 출력층에 클래스 개수만큼 뉴런을 두고 소프트맥스 활성화 함수를 사용한다. 이진 분류일 경우에는 하나의 뉴런을 두고 시그모이드 활성화 함수를 사용한다. 합성곱 신경망과 다른 점은 마지막 셀의 출력이 1차원이기 때문에 Flatten 클래스로 펼칠 필요가 없다. 셀의 출력을 그대로 밀집층에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ccd45",
   "metadata": {},
   "source": [
    "## 09-2 순환 신경망으로 IMDB 리뷰 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ecc5e",
   "metadata": {},
   "source": [
    "## - IMDB 리뷰 데이터셋\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab080aa8",
   "metadata": {},
   "source": [
    "IMDB 리뷰 데이터셋은 유명한 인터넷 영화 데이터베이스인 imdb.com에서 수집한 리뷰를 감상평에 따라 긍정과 부정으로 분류해 놓은 데이터셋이다. 총 50,000개의 샘플로 이루어져 있고 훈련 데이터와 테스트 데이터에 각각 25,000개씩 나누어져 있다. 사실 텍스트 자체를 신경망에 전달하지는 않는다. 컴퓨터에서 처리하는 모든 것은 어떤 숫자 데이터이다. 앞서 합성곱 신경망에서 이미지를 다룰 때는 특별한 변환을 하지 않았다. 이미지가 정수 픽셀값으로 이루어져 있기 때문이다. 텍스트 데이터의 경우 단어를 숫자 데이터로 바꾸는 일반적인 방법은 데이터에 등장하는 단어마다 고유의 정수를 부여하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026bc58",
   "metadata": {},
   "source": [
    "일반적으로 영어 문장은 모두 소문자로 바꾸고 구둣점을 삭제한 다음 공백을 기준으로 분리한다. 이렇게 분리된 단어를 토큰이라고 부른다. 하나의 샘플은 여러 개의 토큰으로 이루어져 있고 1개의 토큰이 하나의 타임스탬프에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09f0ef",
   "metadata": {},
   "source": [
    "토큰에 할당하는 정수 중에 몇 개는 특정한 용도로 예약되어 있는 경우가 많다. 예를 들어 0은 패딩, 1은 문장의 시작, 2는 어휘 사전에 없는 토크을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17aaf3",
   "metadata": {},
   "source": [
    "실제 IMDB 리뷰 데이터셋은 영어로 된 문장이지만 편리하게도 텐서플로에는 이미 정수로 바꾼 데이터가 포함되어 있다. tensorflow.keras.datasets 패키지 아래 imdb 모듈을 임포트하여 이 데이터를 적재해 보자. 전체 데이터셋에서 가장 자주 등장하는 단어 500개만 사용하자. 이렇게 하기 위해 load_data() 함수의 num_words 매개변수를 500으로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed8a94a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe87566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, test_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0deea",
   "metadata": {},
   "source": [
    "훈련 세트와 테스트 세트가 각각 25000개의 샘플로 이루어져 있다. 배열이 1차원인게 좀 이상해 보인다. IMDB 리뷰 텍스트는 길이가 제각가이다. 따라서 고정 크기의 2차원 배열에 담기 보다는 리뷰마다 별도의 파이썬 리스트로 담아야 메모리를 효율적으로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a493df",
   "metadata": {},
   "source": [
    "이 데이터는 개별 리뷰를 담은 파이썬 리스트 객체로 이루어진 넘파이 배열이다. 다음으로 첫 번째 리뷰의 길이를 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1206ee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0ce53",
   "metadata": {},
   "source": [
    "첫 번째 리뷰는 218개의 토큰으로 이루어져 있다. 두 번째 리뷰도 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7fc0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4feb6",
   "metadata": {},
   "source": [
    "이제 첫 번째 리뷰에 담긴 내용을 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f562aec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1012a",
   "metadata": {},
   "source": [
    "이번엔 타깃 데이터를 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322b8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_target[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d93fc8",
   "metadata": {},
   "source": [
    "해결할 문제는 리뷰가 긍정인지 부정인지를 판단하는 것이다. 그러면 이진 분류 문제로 볼 수 있으므로 타깃값이 0과 1로 나누어 진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eca944",
   "metadata": {},
   "source": [
    "데이터를 더 살펴보기 전에 훈련 세트에서 검증 세트를 20%를 떼어 놓자. 그러면 훈련 세트의 크기는 20000개로 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5c1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    train_input, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da46ba",
   "metadata": {},
   "source": [
    "이제 훈련 세트에 대해 몇 가지 조사를 해 보자. 먼저 각 리뷰의 길이를 계산해 넘파이 배열에 담자. 이렇게 하는 이유는 평균적인 리뷰의 길이와 가장 짧은 리뷰의 길이 그리고 가장 긴 리뷰의 길이를 확인하기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf34f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lengths = np.array([len(x) for x in train_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8c3dd",
   "metadata": {},
   "source": [
    "다음으로 넘파이 mean() 함수와 median() 함수를 사용해 리뷰 길이의 평균과 중간값을 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b51f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.00925 178.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lengths), np.median(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9185e",
   "metadata": {},
   "source": [
    "그다음 히스토그램으로 표현해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322cb413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths)\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9bc5c",
   "metadata": {},
   "source": [
    "대부분의 리뷰 길이는 300 미만이다. 평균이 중간값보다 높은 이유는 오른쪽 끝에 아주 큰 데이터가 있기 때문이다. 리뷰는 대부분 짧아서 이 예제에서는 100개의 단어만 사용하자. 하지만 여전히 100개의 단어보다 작은 리뷰가 있다. 이런 리뷰들의 길이를 100에 맞추기 위해 패딩이 필요하다. 보통 패딩을 나타내는 토큰으로는 0을 사용한다.\n",
    "케라스의 pad_sequences() 함수를 사용해 train_input의 길이를 100으로 맞추자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9361f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_seq = pad_sequences(train_input, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d48678e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35aa29",
   "metadata": {},
   "source": [
    "이제 train_seq에 있는 첫 번째 샘플을 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6eaf3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10   4  20   9   2 364 352   5  45   6   2   2  33 269   8   2 142   2\n",
      "   5   2  17  73  17 204   5   2  19  55   2   2  92  66 104  14  20  93\n",
      "  76   2 151  33   4  58  12 188   2 151  12 215  69 224 142  73 237   6\n",
      "   2   7   2   2 188   2 103  14  31  10  10 451   7   2   5   2  80  91\n",
      "   2  30   2  34  14  20 151  50  26 131  49   2  84  46  50  37  80  79\n",
      "   6   2  46   7  14  20  10  10 470 158]\n"
     ]
    }
   ],
   "source": [
    "print(train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67310927",
   "metadata": {},
   "source": [
    "이 샘플의 앞뒤에 패딩값 0이 없는 것으로 보아 100보다는 길었을 것 같다. 원래 샘플에 앞이 잘렸는지 뒷부분이 잘렸는지 확인하기 위해 trian_input에 있는 원본 샘플의 끝을 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3808ba5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 46, 7, 14, 20, 10, 10, 470, 158]\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f62eb",
   "metadata": {},
   "source": [
    "뒷부분의 출력값이 정확히 일치한다. 그렇다면 앞부분이 잘렸다는 것을 짐작할 수 있다. pad_sequence() 함수는 기본으로 maxlen보다 긴 시퀀스의 앞부분을 자른다. 이렇게 하는 이유는 일반적으로 시퀀스의 뒷부분의 정보가 더 유용하리라 기대하기 때문이다. 이번에는 train_seq에 있는 여섯 번째 샘플을 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c40bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   1   2 195  19  49   2   2 190   4   2 352   2 183  10\n",
      "  10  13  82  79   4   2  36  71 269   8   2  25  19  49   7   4   2   2\n",
      "   2   2   2  10  10  48  25  40   2  11   2   2  40   2   2   5   4   2\n",
      "   2  95  14 238  56 129   2  10  10  21   2  94 364 352   2   2  11 190\n",
      "  24 484   2   7  94 205 405  10  10  87   2  34  49   2   7   2   2   2\n",
      "   2   2 290   2  46  48  64  18   4   2]\n"
     ]
    }
   ],
   "source": [
    "print(train_seq[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124c889",
   "metadata": {},
   "source": [
    "앞부분에 0이 있는 것으로 보아 이 샘플의 길이는 100이 안되는 걸로 보인다. 역시 같은 이유로 패딩 토큰은 시퀀스의 뒷부분이 아니라 앞부분에 추가된다. 시퀀스의 마지막에 있는 단어가 셀의 은닉 상태에 가장 큰 영향을 미치게 되므로 마지막에 패딩을 추가하는 것은 일반적으로 선호하지 않는다. 그러면 이 방식대로 검증 세트의 길이도 100으로 맞추어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc74f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq = pad_sequences(val_input, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a78fe",
   "metadata": {},
   "source": [
    "### - 순환 신경망 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c6d95",
   "metadata": {},
   "source": [
    "케라스는 여러 종류의 순환층 클래스를 제공한다. 그중에 가장 간단한 것은 SimpleRNN 클래스이다. IMDB 리뷰 문제는 이진 분류이므로 마지막 출력층은 1개의 뉴런을 가지고 시그모이드 활성화 함수를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de115fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3324397",
   "metadata": {},
   "source": [
    "이 코드는 지금까지 보았던 구성과 매우 비슷하다. 달라진 것은 Dense나 Conv2D 클래스 대신 SimpleRNN 클래스를 사용했다. 첫 번째 매개변수는 사용할 뉴런의 개수로 input_shape에 입력 차원을 (100,500)으로 지정했다. SimpleRNN 클래스의 activation 매개변수의 기본값은 tanh로 하이퍼볼릭 탄젠트 함수를 사용한다. 여기선 기본값 그대로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444e37d",
   "metadata": {},
   "source": [
    "정수값에 있는 크기 속성을 없애고 각 정수를 고유하게 표현하는 방법은 원-한 인코딩이다. 처음 imdb.load_data() 함수에서 500개 단어만 사용하도록 지정했기 때문에 고유한 단어는 모두 500개이다. 즉 훈련 데이터에 포함될 수 있는 정숫값의 범위는 0에서 499 까지이다. 따라서 이 범위를 원-핫 인코딩으로 표현하려면 배열의 길이가 500이어야 한다. 케라스에서는 keras.utils 패키지 아래에 있는 to_categorical() 함수를 이용하면 원-핫 인코딩된 배열을 반환해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4491d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = keras.utils.to_categorical(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a770f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100, 500)\n"
     ]
    }
   ],
   "source": [
    "print(train_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e7d4d",
   "metadata": {},
   "source": [
    "train_oh의 첫 번째 샘플의 첫 번째 토큰 10이 잘 인코딩 되었는지 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c1a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_oh[0][0][:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa87ed3",
   "metadata": {},
   "source": [
    "처음 12개의 원소를 출력해 보면 열 한번째 원소가 1인것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ead7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_oh[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65a220",
   "metadata": {},
   "source": [
    "열한 번째 원소만 1이고 나머지는 모두 0이어서 원-핫 인코딩된 배열의 값을 모두 더한 결과가 1이 되었다. 같은 방식으로 val_seq도 원-핫 인코딩으로 바꾸어놓자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa6e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_oh = keras.utils.to_categorical(val_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c6622",
   "metadata": {},
   "source": [
    "이제 훈련에 사용할 훈련 세트와 검증 세트가 모두 준비되었다. 앞서 만든 모델의 구조를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d82f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 8)                 4072      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 4,081\n",
      "Trainable params: 4,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef87ecf",
   "metadata": {},
   "source": [
    "### - 순환 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e8725",
   "metadata": {},
   "source": [
    "순환 신경망의 훈련은 완전 연결 신경망이나 합성곱 신경망과 크게 다르지 않다. 모델을 만드는 것은 달라도 훈련하는 방법은 모두 같다. 다음 코드처럼 모델을 컴파일하고 훈련하는 전체 구조가 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6bd0df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 25s 1ms/sample - loss: 0.6985 - acc: 0.4954 - val_loss: 0.6975 - val_acc: 0.4998\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 15s 750us/sample - loss: 0.6955 - acc: 0.5036 - val_loss: 0.6955 - val_acc: 0.5040\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 10s 501us/sample - loss: 0.6935 - acc: 0.5094 - val_loss: 0.6940 - val_acc: 0.5042\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 9s 466us/sample - loss: 0.6918 - acc: 0.5192 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 10s 477us/sample - loss: 0.6896 - acc: 0.5325 - val_loss: 0.6881 - val_acc: 0.5348\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 9s 453us/sample - loss: 0.6815 - acc: 0.5755 - val_loss: 0.6788 - val_acc: 0.5870\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 9s 453us/sample - loss: 0.6710 - acc: 0.6092 - val_loss: 0.6687 - val_acc: 0.6104\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 9s 447us/sample - loss: 0.6569 - acc: 0.6420 - val_loss: 0.6527 - val_acc: 0.6566\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 9s 439us/sample - loss: 0.6406 - acc: 0.6740 - val_loss: 0.6365 - val_acc: 0.6776\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 9s 434us/sample - loss: 0.6236 - acc: 0.6946 - val_loss: 0.6216 - val_acc: 0.6940\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 9s 426us/sample - loss: 0.6065 - acc: 0.7116 - val_loss: 0.6059 - val_acc: 0.7120\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 9s 430us/sample - loss: 0.5902 - acc: 0.7257 - val_loss: 0.5912 - val_acc: 0.7218\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 9s 464us/sample - loss: 0.5741 - acc: 0.7384 - val_loss: 0.5774 - val_acc: 0.7294\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 10s 486us/sample - loss: 0.5585 - acc: 0.7488 - val_loss: 0.5633 - val_acc: 0.7406\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 10s 500us/sample - loss: 0.5442 - acc: 0.7569 - val_loss: 0.5513 - val_acc: 0.7482\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 9s 475us/sample - loss: 0.5312 - acc: 0.7646 - val_loss: 0.5395 - val_acc: 0.7556\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 9s 443us/sample - loss: 0.5191 - acc: 0.7707 - val_loss: 0.5293 - val_acc: 0.7588\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 8s 422us/sample - loss: 0.5080 - acc: 0.7761 - val_loss: 0.5188 - val_acc: 0.7684\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 9s 461us/sample - loss: 0.4984 - acc: 0.7804 - val_loss: 0.5140 - val_acc: 0.7652\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 10s 516us/sample - loss: 0.4893 - acc: 0.7850 - val_loss: 0.5039 - val_acc: 0.7722\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 10s 487us/sample - loss: 0.4811 - acc: 0.7879 - val_loss: 0.4970 - val_acc: 0.7780\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 10s 513us/sample - loss: 0.4739 - acc: 0.7903 - val_loss: 0.4921 - val_acc: 0.7770\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 11s 533us/sample - loss: 0.4670 - acc: 0.7943 - val_loss: 0.4869 - val_acc: 0.7792\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 11s 572us/sample - loss: 0.4610 - acc: 0.7941 - val_loss: 0.4817 - val_acc: 0.7806\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 12s 576us/sample - loss: 0.4554 - acc: 0.7984 - val_loss: 0.4779 - val_acc: 0.7808\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 10s 515us/sample - loss: 0.4500 - acc: 0.8018 - val_loss: 0.4758 - val_acc: 0.7810\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 9s 467us/sample - loss: 0.4456 - acc: 0.8030 - val_loss: 0.4722 - val_acc: 0.7830\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 10s 480us/sample - loss: 0.4414 - acc: 0.8036 - val_loss: 0.4694 - val_acc: 0.7856\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 10s 488us/sample - loss: 0.4377 - acc: 0.8067 - val_loss: 0.4690 - val_acc: 0.7838\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 9s 462us/sample - loss: 0.4344 - acc: 0.8073 - val_loss: 0.4660 - val_acc: 0.7858\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 10s 476us/sample - loss: 0.4312 - acc: 0.8083 - val_loss: 0.4633 - val_acc: 0.7890\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 10s 492us/sample - loss: 0.4284 - acc: 0.8111 - val_loss: 0.4629 - val_acc: 0.7872\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 10s 482us/sample - loss: 0.4260 - acc: 0.8130 - val_loss: 0.4599 - val_acc: 0.7912\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 9s 450us/sample - loss: 0.4235 - acc: 0.8131 - val_loss: 0.4603 - val_acc: 0.7902\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 10s 500us/sample - loss: 0.4213 - acc: 0.8146 - val_loss: 0.4617 - val_acc: 0.7872\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 10s 480us/sample - loss: 0.4194 - acc: 0.8159 - val_loss: 0.4569 - val_acc: 0.7894\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 9s 428us/sample - loss: 0.4179 - acc: 0.8158 - val_loss: 0.4570 - val_acc: 0.7904\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 9s 431us/sample - loss: 0.4161 - acc: 0.8165 - val_loss: 0.4569 - val_acc: 0.7904\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 9s 451us/sample - loss: 0.4144 - acc: 0.8184 - val_loss: 0.4570 - val_acc: 0.7900\n"
     ]
    }
   ],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', \n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_oh, train_target, epochs=100, batch_size=64,\n",
    "                    validation_data=(val_oh, val_target),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84510c59",
   "metadata": {},
   "source": [
    "이 훈련은 43 번째 에포크에서 조기 종료되었다. 검증 세트에 대한 정확도는 약 80% 정도이다. 매우 뛰어난 성능은 아니지만 감상평을 분류하는 데 어느 정도 성과를 내고 있다고 판단할 수 있다. 그럼 이전 장에서처럼 검증 손실을 그래프로 그려서 훈련 과정을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c516ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70502d8",
   "metadata": {},
   "source": [
    "훈련 손실은 꾸준히 감소하고 있지만 검증 손실은 대략 스무 번째 에포크에서 감소가 둔해지고 있다. 적절한 에포크에서 훈련을 멈춘 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76515eff",
   "metadata": {},
   "source": [
    "### - 단어 임베딩을 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c7964",
   "metadata": {},
   "source": [
    "순환 신경망에서 텍스트를 처리할 때 즐겨 사용하는 방법은 단어 임베딩이다. 단어 임베딩은 각 단어를 고정된 크기의 실수 벡터로 바꾸어 준다. 이런 단어 임베딩으로 만들어진 벡터는 원-핫 인코딩된 벡터보다 훨씬 의미 있는 값으로 채워져 있기 때문에 자연어 처리에서 더 좋은 성능을 내는 경우가 많다. 물론 이런 단어 임베딩 벡터를 만드는 층은 이미 준비되어 있다. 케라스에서는 keras.layers 패키지 아래 Embedding 클래스로 임베딩 기능을 제공한다. 이 클래스를 다른 층처럼 모델에 추가하면 처음에는 모든 벡터가 랜덤하게 초기화 되지만 훈련을 통해 데이터에서 좋은 단어 임베딩을 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54581bb",
   "metadata": {},
   "source": [
    "단어 임베딩의 장점은 입력으로 정수 데이터를 받는다는 것이다. 즉 원-핫 인코딩으로 변경된 train_oh 배열이 아니라 train_seq를 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b7049",
   "metadata": {},
   "source": [
    "Embbeding 클래스를 SimpleRNN층 앞에 추가한 두 번째 순환 신경망을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04bf7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
    "model2.add(keras.layers.SimpleRNN(8))\n",
    "model2.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7c425",
   "metadata": {},
   "source": [
    "이 모델의 구조를 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc753558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           8000      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 8)                 200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 8,209\n",
      "Trainable params: 8,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea4c4e",
   "metadata": {},
   "source": [
    "Embedding의 모델 훈련 과정은 이전과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ebbd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 6s 312us/sample - loss: 0.6953 - acc: 0.5103 - val_loss: 0.6931 - val_acc: 0.5102\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 6s 278us/sample - loss: 0.6811 - acc: 0.5796 - val_loss: 0.6717 - val_acc: 0.6300\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 6s 279us/sample - loss: 0.6599 - acc: 0.6622 - val_loss: 0.6538 - val_acc: 0.6798\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 6s 285us/sample - loss: 0.6407 - acc: 0.7027 - val_loss: 0.6350 - val_acc: 0.7128\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 6s 288us/sample - loss: 0.6216 - acc: 0.7285 - val_loss: 0.6190 - val_acc: 0.7248\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 6s 280us/sample - loss: 0.6039 - acc: 0.7444 - val_loss: 0.6104 - val_acc: 0.7268\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 6s 280us/sample - loss: 0.5869 - acc: 0.7593 - val_loss: 0.5902 - val_acc: 0.7442\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 5s 260us/sample - loss: 0.5702 - acc: 0.7656 - val_loss: 0.5789 - val_acc: 0.7424\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 5s 274us/sample - loss: 0.5550 - acc: 0.7743 - val_loss: 0.5611 - val_acc: 0.7630\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 5s 273us/sample - loss: 0.5404 - acc: 0.7796 - val_loss: 0.5515 - val_acc: 0.7604\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 6s 298us/sample - loss: 0.5272 - acc: 0.7864 - val_loss: 0.5397 - val_acc: 0.7658\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 6s 280us/sample - loss: 0.5144 - acc: 0.7898 - val_loss: 0.5298 - val_acc: 0.7680\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 5s 273us/sample - loss: 0.5039 - acc: 0.7936 - val_loss: 0.5210 - val_acc: 0.7704\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 8s 387us/sample - loss: 0.4933 - acc: 0.7970 - val_loss: 0.5140 - val_acc: 0.7734\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 6s 286us/sample - loss: 0.4838 - acc: 0.7998 - val_loss: 0.5066 - val_acc: 0.7786\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 6s 299us/sample - loss: 0.4758 - acc: 0.8029 - val_loss: 0.5026 - val_acc: 0.7740\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 6s 279us/sample - loss: 0.4676 - acc: 0.8050 - val_loss: 0.4949 - val_acc: 0.7800\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 5s 271us/sample - loss: 0.4601 - acc: 0.8079 - val_loss: 0.4914 - val_acc: 0.7788\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 6s 277us/sample - loss: 0.4541 - acc: 0.8094 - val_loss: 0.4892 - val_acc: 0.7802\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 5s 267us/sample - loss: 0.4477 - acc: 0.8122 - val_loss: 0.4872 - val_acc: 0.7798\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 5s 260us/sample - loss: 0.4429 - acc: 0.8133 - val_loss: 0.4841 - val_acc: 0.7792\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 7s 364us/sample - loss: 0.4377 - acc: 0.8145 - val_loss: 0.4786 - val_acc: 0.7820\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 7s 337us/sample - loss: 0.4324 - acc: 0.8163 - val_loss: 0.4771 - val_acc: 0.7842\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 6s 284us/sample - loss: 0.4281 - acc: 0.8187 - val_loss: 0.4759 - val_acc: 0.7820\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 6s 281us/sample - loss: 0.4246 - acc: 0.8197 - val_loss: 0.4747 - val_acc: 0.7818\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 6s 280us/sample - loss: 0.4205 - acc: 0.8224 - val_loss: 0.4733 - val_acc: 0.7838\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 6s 282us/sample - loss: 0.4169 - acc: 0.8237 - val_loss: 0.4765 - val_acc: 0.7810\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 6s 276us/sample - loss: 0.4141 - acc: 0.8253 - val_loss: 0.4717 - val_acc: 0.7816\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 6s 283us/sample - loss: 0.4109 - acc: 0.8253 - val_loss: 0.4754 - val_acc: 0.7820\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 6s 277us/sample - loss: 0.4084 - acc: 0.8266 - val_loss: 0.4720 - val_acc: 0.7836\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 6s 282us/sample - loss: 0.4062 - acc: 0.8272 - val_loss: 0.4708 - val_acc: 0.7836\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 6s 295us/sample - loss: 0.4036 - acc: 0.8283 - val_loss: 0.4725 - val_acc: 0.7816\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 6s 282us/sample - loss: 0.4016 - acc: 0.8295 - val_loss: 0.4729 - val_acc: 0.7826\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 6s 277us/sample - loss: 0.3993 - acc: 0.8309 - val_loss: 0.4716 - val_acc: 0.7822\n"
     ]
    }
   ],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', \n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
    "                     validation_data=(val_seq, val_target),\n",
    "                     callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505034c",
   "metadata": {},
   "source": [
    "출력 결과를 보면 원-핫 인코딩을 사용한 모델과 비슷한 성능을 냈다. 반면에 순환층의 가중치 개수는 훨씬 작고 훈련 세트의 크기도 훨씬 줄어 들었다. 마지막으로 훈련 손실과 검증 손실을 그래프로 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0972502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46da862",
   "metadata": {},
   "source": [
    "검증 손실이 더 감소되지 않아 훈련이 적절히 조기 종료된 것 같다. 이에 비해 훈련 손실은 계속 감소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3ed37",
   "metadata": {},
   "source": [
    "## 09-3 LSTM과 GRU 셀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7da046",
   "metadata": {},
   "source": [
    "### - LSTM 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c541e9a",
   "metadata": {},
   "source": [
    "LSTM은 Long Short-Term Memory의 약자이다. 말 그대로 단기 기억을 오래 기억하기 위해 고안되었다. LSTM은 구조가 복잡하므로 단계적으로 설명하겠다. 하지만 기본 개념은 동일하다. LSTM에는 입력과 가중치를 곱하고 절편을 더해 활성화 함수를 통과시키는 구조를 여러 개 가지고 있다. 이런 계산 결과는 다음 타임스텝에 재사용된다. 먼저 은닉상태를 만드는 방법을 알아보자. 은닉 상태는 입력과 이전 타임스텝의 은닉 상태를 가중치에 곱한 후 활성화 함수를 통과시켜 다음 은닉 상태를 만든다. 이때 기본 순환층과는 달리 시그모이드 활성화 함수를 사용한다. 또 tanh 활성화 함수를 통과한 어떤 값과 곱해져서 은닉 상태를 만든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d26a0",
   "metadata": {},
   "source": [
    "LSTM에는 순환되는 상태가 2개이다. 은닉 상태 말고 셀 상태라고 부르는 값이 또 있다. 은닉 상태와 달리 셀 상태는 다음 층으로 전달되지 않고 LSTM 셀에서 순환만 되는 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc04a8",
   "metadata": {},
   "source": [
    "셀 상태를 계산하는 과정은 먼저 입력과 은닉 상태를 또 다른 가중치 wf에 곱한 다음 시그모이드 함수를 통과시킨다. 그다음 이전 타임스테의 셀 상태와 곱하여 새로운 셀 상태를 만든다. 이 셀 상태가 오른쪽에서 tanh 함수를 통과하여 새로운 은닉 상태를 만드는 데 기여한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2068d4",
   "metadata": {},
   "source": [
    "### - LSTM 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dd2b8",
   "metadata": {},
   "source": [
    "먼저 이전 절에서처럼 IMDB 리뷰 데이터를 로드하고 훈련 세트와 검증 세트로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f189111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/jaeyoon89/.local/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)\n",
    "\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    train_input, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4039b",
   "metadata": {},
   "source": [
    "그 다음 케라스의 pad_sequences() 함수로 각 샘플의 길이를 100에 맞추고 부족할 때는 패딩을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5479a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_seq = pad_sequences(train_input, maxlen=100)\n",
    "val_seq = pad_sequences(val_input, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a2a61",
   "metadata": {},
   "source": [
    "이제 LSTM 셀을 사용한 순환층을 만들어 보자. 사실 SimpleRnn 클래스를 LSTM 클래스로 바꾸기만 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fca71017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(500, 16, input_length=100))\n",
    "model.add(keras.layers.LSTM(8))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c26e49",
   "metadata": {},
   "source": [
    "모델 구조를 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1d8785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 16)           8000      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 8,809\n",
      "Trainable params: 8,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c995b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 13s 653us/sample - loss: 0.6923 - acc: 0.5436 - val_loss: 0.6912 - val_acc: 0.5882\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 14s 686us/sample - loss: 0.6897 - acc: 0.6084 - val_loss: 0.6879 - val_acc: 0.6268\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 14s 699us/sample - loss: 0.6842 - acc: 0.6470 - val_loss: 0.6801 - val_acc: 0.6638\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 14s 713us/sample - loss: 0.6695 - acc: 0.6804 - val_loss: 0.6548 - val_acc: 0.6976\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 15s 768us/sample - loss: 0.6111 - acc: 0.7244 - val_loss: 0.5717 - val_acc: 0.7258\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 18s 912us/sample - loss: 0.5526 - acc: 0.7391 - val_loss: 0.5447 - val_acc: 0.7444\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 18s 914us/sample - loss: 0.5291 - acc: 0.7566 - val_loss: 0.5267 - val_acc: 0.7542\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 18s 920us/sample - loss: 0.5093 - acc: 0.7674 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 18s 908us/sample - loss: 0.4925 - acc: 0.7770 - val_loss: 0.4940 - val_acc: 0.7738\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 18s 880us/sample - loss: 0.4775 - acc: 0.7875 - val_loss: 0.4828 - val_acc: 0.7786\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 18s 911us/sample - loss: 0.4662 - acc: 0.7922 - val_loss: 0.4740 - val_acc: 0.7824\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 18s 906us/sample - loss: 0.4561 - acc: 0.7962 - val_loss: 0.4659 - val_acc: 0.7888\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 18s 885us/sample - loss: 0.4483 - acc: 0.8005 - val_loss: 0.4611 - val_acc: 0.7862\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 18s 889us/sample - loss: 0.4422 - acc: 0.8024 - val_loss: 0.4556 - val_acc: 0.7890\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 18s 883us/sample - loss: 0.4368 - acc: 0.8053 - val_loss: 0.4537 - val_acc: 0.7896\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 18s 875us/sample - loss: 0.4325 - acc: 0.8092 - val_loss: 0.4492 - val_acc: 0.7906\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 18s 890us/sample - loss: 0.4289 - acc: 0.8094 - val_loss: 0.4470 - val_acc: 0.7924\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 20s 976us/sample - loss: 0.4256 - acc: 0.8117 - val_loss: 0.4456 - val_acc: 0.7938\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 18s 896us/sample - loss: 0.4229 - acc: 0.8130 - val_loss: 0.4443 - val_acc: 0.7940\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 19s 966us/sample - loss: 0.4210 - acc: 0.8134 - val_loss: 0.4407 - val_acc: 0.7966\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 18s 893us/sample - loss: 0.4185 - acc: 0.8138 - val_loss: 0.4390 - val_acc: 0.7974\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 18s 885us/sample - loss: 0.4171 - acc: 0.8144 - val_loss: 0.4390 - val_acc: 0.7974\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 18s 904us/sample - loss: 0.4154 - acc: 0.8144 - val_loss: 0.4362 - val_acc: 0.7974\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 17s 868us/sample - loss: 0.4136 - acc: 0.8162 - val_loss: 0.4356 - val_acc: 0.7974\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 18s 907us/sample - loss: 0.4125 - acc: 0.8145 - val_loss: 0.4336 - val_acc: 0.8002\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 18s 902us/sample - loss: 0.4109 - acc: 0.8164 - val_loss: 0.4337 - val_acc: 0.7986\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 18s 906us/sample - loss: 0.4097 - acc: 0.8168 - val_loss: 0.4325 - val_acc: 0.7986\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 18s 898us/sample - loss: 0.4089 - acc: 0.8166 - val_loss: 0.4316 - val_acc: 0.8004\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 17s 865us/sample - loss: 0.4076 - acc: 0.8171 - val_loss: 0.4310 - val_acc: 0.8010\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 17s 852us/sample - loss: 0.4068 - acc: 0.8174 - val_loss: 0.4348 - val_acc: 0.7930\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 17s 861us/sample - loss: 0.4059 - acc: 0.8173 - val_loss: 0.4291 - val_acc: 0.8030\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 17s 846us/sample - loss: 0.4054 - acc: 0.8192 - val_loss: 0.4291 - val_acc: 0.8040\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 18s 924us/sample - loss: 0.4044 - acc: 0.8183 - val_loss: 0.4276 - val_acc: 0.8034\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 18s 891us/sample - loss: 0.4033 - acc: 0.8193 - val_loss: 0.4270 - val_acc: 0.8034\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 19s 927us/sample - loss: 0.4027 - acc: 0.8195 - val_loss: 0.4278 - val_acc: 0.8054\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 19s 932us/sample - loss: 0.4018 - acc: 0.8201 - val_loss: 0.4273 - val_acc: 0.8036\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 18s 906us/sample - loss: 0.4012 - acc: 0.8207 - val_loss: 0.4261 - val_acc: 0.8036\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 17s 873us/sample - loss: 0.4004 - acc: 0.8213 - val_loss: 0.4248 - val_acc: 0.8056\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 18s 896us/sample - loss: 0.3996 - acc: 0.8221 - val_loss: 0.4247 - val_acc: 0.8056\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 18s 916us/sample - loss: 0.3988 - acc: 0.8205 - val_loss: 0.4244 - val_acc: 0.8056\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 18s 907us/sample - loss: 0.3986 - acc: 0.8202 - val_loss: 0.4253 - val_acc: 0.8042\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 20s 985us/sample - loss: 0.3979 - acc: 0.8210 - val_loss: 0.4255 - val_acc: 0.8052\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 18s 898us/sample - loss: 0.3973 - acc: 0.8217 - val_loss: 0.4234 - val_acc: 0.8064\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 18s 914us/sample - loss: 0.3964 - acc: 0.8220 - val_loss: 0.4236 - val_acc: 0.8074\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 19s 942us/sample - loss: 0.3960 - acc: 0.8213 - val_loss: 0.4231 - val_acc: 0.8084\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 18s 916us/sample - loss: 0.3953 - acc: 0.8215 - val_loss: 0.4232 - val_acc: 0.8070\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 18s 899us/sample - loss: 0.3944 - acc: 0.8231 - val_loss: 0.4228 - val_acc: 0.8074\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 18s 886us/sample - loss: 0.3939 - acc: 0.8220 - val_loss: 0.4228 - val_acc: 0.8068\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 18s 904us/sample - loss: 0.3935 - acc: 0.8216 - val_loss: 0.4209 - val_acc: 0.8086\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 18s 900us/sample - loss: 0.3928 - acc: 0.8228 - val_loss: 0.4216 - val_acc: 0.8076\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 18s 876us/sample - loss: 0.3924 - acc: 0.8239 - val_loss: 0.4235 - val_acc: 0.8066\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 18s 901us/sample - loss: 0.3918 - acc: 0.8249 - val_loss: 0.4205 - val_acc: 0.8074\n",
      "Epoch 53/100\n",
      "12608/20000 [=================>............] - ETA: 6s - loss: 0.3874 - acc: 0.8251"
     ]
    }
   ],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', \n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
    "                    validation_data=(val_seq, val_target),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041aedb",
   "metadata": {},
   "source": [
    "훈련 손실과 검증 손실 그래프를 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc768da4",
   "metadata": {},
   "source": [
    "그래프를 보면 기본 순환층 보다 LSTM이 과대 적합을 억제하면서 훈련을 잘 수행한 것으로 보인다. 하지만 경우에 따라서는 과대적합을 더 강하게 제어할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1362b70",
   "metadata": {},
   "source": [
    "### - 순환층에 드롭아웃 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f24e8",
   "metadata": {},
   "source": [
    "완전 연결 신경망과 합성곱 신경망에서는 Dropout 클래스를 사용해 드롭아웃을 적용했다. 이를 통해 모델이 훈련 세트에 너무 과대적합 되는 것을 막았다. 순환층은 자체적으로 드롭아웃 기능을 제공한다. SimpleRNN과 LSTM 클래스 모두 dropout 매개변수와 recurrent_dropout 매개변수를 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e229f09",
   "metadata": {},
   "source": [
    "dropout 매개변수는 셀의 입력에 드롭아웃을 적용하고 recurrent_dropout은 순환되는 은닉 상태에 드롭아웃을 적용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddc3a6",
   "metadata": {},
   "source": [
    "LSTM 클래스에 dropout 매개변수를 0.3으로 지정하여 30%의 입력을 드롭아웃한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0077ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential()\n",
    "\n",
    "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
    "model2.add(keras.layers.LSTM(8, dropout=0.3))\n",
    "model2.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d5e21",
   "metadata": {},
   "source": [
    "이 모델을 이전과 동일한 조건으로 훈련해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-dropout-model.h5', \n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
    "                     validation_data=(val_seq, val_target),\n",
    "                     callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff192f3",
   "metadata": {},
   "source": [
    "검증 손실이 약간 향상된 것 같다. 그래프로 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fa360",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c3d64",
   "metadata": {},
   "source": [
    "LSTM 층에 적용한 드롭아웃이 효과를 발휘한 것 같다. 훈련 손실과 검증 손실 같의 차이가 좁혀진 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98f59a",
   "metadata": {},
   "source": [
    "### - 2개의 층을 연결하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c244c",
   "metadata": {},
   "source": [
    "순환층을 연결할 때는 한 가지 주의할 점이 있다. 앞서 언급했지만 순환층의 은닉 상태는 샘플의 마지막 타임스텝에 대한 은닉 상태만 다음 층으로 전달한다. 하지만 순환층을 쌓게 되면 모든 순환층에 순차 데이터가 필요하다. 따라서 앞쪽의 순환층이 모든 타임스텝에 대한 은닉 상태를 출력해야 한다. 케라스의 순환층에서 모든 타임 스텝의 은닉 상태를 출력하려면 마지막을 제외한 다른 모든 순환층에서 return_sequences 매개변수를 True로 지정하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e28e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.Sequential()\n",
    "\n",
    "model3.add(keras.layers.Embedding(500, 16, input_length=100))\n",
    "model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))\n",
    "model3.add(keras.layers.LSTM(8, dropout=0.3))\n",
    "model3.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28529c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629fbfa",
   "metadata": {},
   "source": [
    "첫 번째 LSTM 층이 모든 타임스텝(100개)의 은닉 상태를 출력하기 때문에 출력 크기가(None,100,8)로 표시되었다. 이에 반해 두 번째 LSTM 층의 출력 크기는 마지막 타임스텝의 은닉 상태만 출력하기 때문에 (None,8)이다. 이 모델을 앞에서와 같이 훈련해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model3.compile(optimizer=rmsprop, loss='binary_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-2rnn-model.h5', \n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "history = model3.fit(train_seq, train_target, epochs=100, batch_size=64,\n",
    "                     validation_data=(val_seq, val_target),\n",
    "                     callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb6639",
   "metadata": {},
   "source": [
    "이제 손실 그래프를 그려서 과대적합이 잘 제어되었는지 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c153dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68fb13",
   "metadata": {},
   "source": [
    "그래프를 보면 과대적합을 잘 제어하면서 손실을 최대한 낮춘 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4950fc6",
   "metadata": {},
   "source": [
    "### - GRU 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08286392",
   "metadata": {},
   "source": [
    "GRU는 Gated Recurrent Unit의 약자이다 이 셀은 LSTM을 간소화한 버전으로 생각할 수 있다. 이 셀은 LSTM처럼 셀 상태를 계산하지 않고 은닉 상태 하나만 포함하고 있다. GRU 셀에는 은닉 상태와 입력에 가중치를 곱하고 절편을 더하는 작은 셀이 3개 들어 있다. 2개는 시그모이드 활성화 함수를 사용하고 하나는 tanh 함수르 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
